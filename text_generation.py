# -*- coding: utf-8 -*-
"""Text generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1haCi7fliyCwU0iuYaqiS-9jPb7OrpHki
"""

pip install tensorflow keras matplotlib

pip install torch transformers datasets

from datasets import load_dataset

# Load a small text dataset for training
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')
texts = dataset['text']

from transformers import GPT2Tokenizer

# Load the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Tokenize the data
def encode(example):
    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)

tokenized_texts = list(map(lambda x: tokenizer.encode(x, truncation=True, max_length=128), texts))

from transformers import GPT2LMHeadModel

# Load the GPT-2 model
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.train()  # Put the model in training mode

from torch.utils.data import DataLoader, Dataset
import torch
from torch.nn.utils.rnn import pad_sequence

# Dummy data - tokenized texts (replace this with your actual tokenized data)
tokenized_texts = [
    [1, 2, 3, 4, 5],  # Example sequence
    [1, 2, 3],
    [1, 2, 3, 4, 5, 6, 7]
]

# Define the Dataset class
class TextDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings)

    def __getitem__(self, idx):
        return torch.tensor(self.encodings[idx])

# Define a collate function for padding
def collate_fn(batch):
    return pad_sequence(batch, batch_first=True, padding_value=0)  # Use 0 as the padding value

# Create a DataLoader with the custom collate function
train_dataset = TextDataset(tokenized_texts)
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)

# Initialize a simple model for demonstration (replace with your actual model)
from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained('gpt2')
model.train()  # Set the model to training mode

# Move the model to CPU
device = torch.device('cpu')
model.to(device)

# Set up the optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Training loop
for epoch in range(3):  # Run for 3 epochs
    for batch in train_loader:
        inputs = batch.to(device)  # Move input to CPU
        outputs = model(inputs, labels=inputs)
        loss = outputs.loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch + 1} completed with loss: {loss.item()}')

# Save the fine-tuned model
model.save_pretrained('./fine-tuned-gpt2')

